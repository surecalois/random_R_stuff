---
title: "Stat note"
author: "xx"
date: "January 8, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Chi-square test
- degree of freedom = (row - 1)*(col - 1), choose one factor when the other is 1
- critical value

##example 1
flip a conin for 100 time, and it turn out 62 heads and 38 tails. Is it a fair coin?
```{r}
x = c(HD = 62, TL = 38)
chisq.test(x)
```

again, flip a conin for 50 time, and it turn out 28 heads and 22 tails. Is it a fair coin?
```{r}
x = c(HD = 28, TL = 22)
chisq.test(x)
```

##example 2
roll a die 36 times, we got 1:2, 2:4, 3:8, 4:9,5:3, 6:10. Is it a fair die?
```{r}
x = c(2,4,8,9,3,10)
chisq.test(x)
```

##example 3
observed table
```{r echo = F}
M <- as.table(rbind(c(26, 13, 5), c(20, 29, 7)))
dimnames(M) <- list(c("up", "down"),c("L","C", "R"))
print(M)
rowSums(M)
colSums(M)
```

the expected table (but you don't need it to do it yourself)
take down C for example, two ways

A: there are 56 (= 20+29+7) downs, and 42% (=42/(46+42+12)) of C, so 56*42% = 23.52

B: there are 42 (= 13+29) Cs, and 56% (=44/(44+56)) of down, so 42*56% = 23.52
```{r echo = F}
expect_table = function(M){
if(!is.matrix(M))stop("M must be a matrix")
  EM = M
for(i in 1:dim(M)[1]){
  for(j in 1:dim(M)[2]){
    EM[i,j]=sum(M[i,])*sum(M[,j])/sum(M)
  }
}
  return(EM)
  }
EM = expect_table(M)
print(EM)
```

```{r}
chisq.test(M)
```


#fisher excat test
we can begin with sampling $(x_1,x_2,...,x_n)$ from N has $\frac{N!}{x_1!x_2!..x_n!}$ ways. then for 2 dimensional matrix we have.( you should also it can be extented to higher dimension)

$$P = \frac{R_{1}!R_{2}!...R_{m}!C_{1}!C_{2}!...C_{n}!}{N!a_{11}!a_{12}!...a_{1n}!...a_{21}!...a_{mn}!}$$

```{r}
chisq.test(M)
fisher.test(M)
F0 = sum(M)
F1 = c(colSums(M),rowSums(M))
F2 = c(M)
#the configuration probobility is
logP = sum(lfactorial(c(F1))) - sum(lfactorial(c(F0,F2)))
print(exp(logP))
```


#likelihood ratio test
average 3 goals per game? data from 380 games.

```{r}
goals = c(0,1,2,3,4,5,6,7,8,9)
counts = c(30,79,99,67,61,24,11,6,2,1)
print(data.frame(goals=goals,counts = counts))
lambda0 = 3
lambda1 = sum(goals*counts/sum(counts))
plot(goals,dpois(goals,lambda0),ylim = c(0,0.3))
points(goals,dpois(goals,lambda1),col = 'blue')
points(goals,counts/sum(counts),col = 'green')

```

With guessed parameter:lambda, the pobobility of see such result is:
$$P = \prod_{i=0}^{380}poisson(x_i,\lambda)$$
$$ P = \prod_{gl=0}^{9}possion(goal,\lambda)^n$$
$$ LRT = -2*\log(\frac{P_0}{P_1})\sim\chi_{\alpha,1}^2$$
```{r}
logP = function(lambda) {return(sum(counts*dpois(goals,lambda,log=T)))}

LRT = -2*(logP(lambda0)-logP(lambda1))
cat ("LRT = ",LRT," > ", qchisq(1-0.05,1),"\n")
cat( "p-value = ",1-pchisq(LRT,1),"\n")

```

#Z-test and student t-test
when you know $\mu$ and $\sigma$:
$$Z = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\sim N(\mu=0,\sigma=1)$$
when you know $\mu$
$$t = \frac{\bar{X}-\mu}{s/\sqrt{n}} \sim T(\mu=0,df=n-1)$$

```{r}
x = seq(-6,6,0.02)
yn = dnorm(x,0,1)
yt = dt(x,3)
plot(x,yn,type = 'l')
points(x,yt,col = 'blue',type = 'l')
```

###Degree of freedom in two samples
the details are not so nice, because there are different ways to deal with the variance, and thus their relative degree of freedom.

####classical t
$$t = \frac{m_A - m_B}{\sqrt{\frac{s^2}{n_A}+\frac{s^2}{n_B}}}$$
$$s^2=\frac{\sum{(x-m_A)^2}+\sum{(x-m_B)^2}}{n_A+n_B-2}$$
$$df = n_A+n_B-2$$

####t.test use welch t

$$t = \frac{m_A - m_B}{\sqrt{\frac{s_A^2}{n_A}+\frac{s_B^2}{n_B}}}$$
$$s_A^2=\frac{\sum{(x-m_A)^2}}{n_A-1},s_B^2=\frac{\sum{(x-m_B)^2}}{n_B-1}$$
$$df =\frac{\frac{s_A^2}{n_A}+\frac{s_B^2}{n_B}}{\frac{s_A^2}{n_A^2(n_B-1)}+\frac{s_B^2}{n_B^2(n_A-1)}}$$
```{r}
t.test(extra~group,data = sleep)
data(sleep)
print(sleep)
```

